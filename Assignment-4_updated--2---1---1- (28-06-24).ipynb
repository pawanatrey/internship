{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa9aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 .Scrape the details of most viewed videos on YouTube from Wikipedia. \n",
    "# Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "# A) Rank\n",
    "# B) Name\n",
    "# C) Artist\n",
    "# D) Upload date\n",
    "# E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423fc15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807de133",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2c0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"noarticletext\"]/tbody/tr/td/span/a')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b603d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_2 = driver.find_element(By.XPATH,'//*[@id=\"mw-content-text\"]/div[2]/div[4]/ul/li[1]/div/div[2]/div[1]/a')\n",
    "search_2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eeb4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table=driver.find_element(By.XPATH,'//*[@id=\"mw-content-text\"]/div[1]/table[1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57229dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Baby Shark Dance\"[7] Pinkfong Baby Shark - Kids\\' Songs & Stories 14.66 June 17, 2016 [A]', '\"Despacito\"[10] Luis Fonsi 8.47 January 12, 2017 [B]', '\"Johny Johny Yes Papa\"[18] LooLoo Kids - Nursery Rhymes and Children\\'s Songs 6.92 October 8, 2016', '\"Bath Song\"[19] Cocomelon - Nursery Rhymes 6.75 May 2, 2018', '\"See You Again\"[20] Wiz Khalifa 6.30 April 6, 2015 [C]', '\"Shape of You\"[25] Ed Sheeran 6.28 January 30, 2017 [D]', '\"Wheels on the Bus\"[28] Cocomelon - Nursery Rhymes 6.22 May 24, 2018', '\"Phonics Song with Two Words\"[29] ChuChu TV Nursery Rhymes & Kids Songs 5.84 March 6, 2014', '\"Uptown Funk\"[30] Mark Ronson 5.25 November 19, 2014', '\"Gangnam Style\"[31] Psy 5.20 July 15, 2012 [E]', '\"Learning Colors – Colorful Eggs on a Farm\"[36] Miroshka TV 5.13 February 27, 2018', '\"Dame Tu Cosita\"[37] Ultra Records 4.68 April 5, 2018', '\"Axel F\"[38] Crazy Frog 4.60 June 16, 2009', '\"Masha and the Bear – Recipe for Disaster\"[39] Get Movies 4.60 January 31, 2012', '\"Baa Baa Black Sheep\"[40] Cocomelon - Nursery Rhymes 4.10 June 25, 2018', '\"Lakdi Ki Kathi\"[41] Jingle Toons 4.07 June 14, 2018', '\"Sugar\"[42] Maroon 5 4.06 January 14, 2015', '\"Counting Stars\"[43] OneRepublic 4.03 May 31, 2013', '\"Roar\"[44] Katy Perry 4.01 September 5, 2013', '\"Waka Waka (This Time for Africa)\"[45] Shakira 3.96 June 4, 2010', '\"Shree Hanuman Chalisa\"[46] T-Series Bhakti Sagar 3.90 May 10, 2011', '\"Humpty the train on a fruits ride\"[47] Kiddiestv Hindi - Nursery Rhymes & Kids Songs 3.84 January 26, 2018', '\"Sorry\"[48] Justin Bieber 3.82 October 22, 2015', '\"Thinking Out Loud\"[49] Ed Sheeran 3.78 October 7, 2014', '\"Perfect\"[50] Ed Sheeran 3.75 November 9, 2017', '\"Dark Horse\"[51] Katy Perry 3.74 February 20, 2014', '\"Let Her Go\"[52] Passenger 3.68 July 25, 2012', '\"Faded\"[53] Alan Walker 3.65 December 3, 2015', '\"Girls Like You\"[54] Maroon 5 3.62 May 31, 2018', '\"Lean On\"[55] Major Lazer Official 3.62 March 22, 2015']\n"
     ]
    }
   ],
   "source": [
    "# Total Rows in table\n",
    "rows=[]\n",
    "table_rows=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr')\n",
    "for i in table_rows:\n",
    "    rows.append(i.text)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ac905f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9224e985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Baby Shark Dance\"[7]', \"Pinkfong Baby Shark - Kids' Songs & Stories\", '14.66', 'June 17, 2016', '[A]', '\"Despacito\"[10]', 'Luis Fonsi', '8.47', 'January 12, 2017', '[B]', '\"Johny Johny Yes Papa\"[18]', \"LooLoo Kids - Nursery Rhymes and Children's Songs\", '6.92', 'October 8, 2016', '', '\"Bath Song\"[19]', 'Cocomelon - Nursery Rhymes', '6.75', 'May 2, 2018', '', '\"See You Again\"[20]', 'Wiz Khalifa', '6.30', 'April 6, 2015', '[C]', '\"Shape of You\"[25]', 'Ed Sheeran', '6.28', 'January 30, 2017', '[D]', '\"Wheels on the Bus\"[28]', 'Cocomelon - Nursery Rhymes', '6.22', 'May 24, 2018', '', '\"Phonics Song with Two Words\"[29]', 'ChuChu TV Nursery Rhymes & Kids Songs', '5.84', 'March 6, 2014', '', '\"Uptown Funk\"[30]', 'Mark Ronson', '5.25', 'November 19, 2014', '', '\"Gangnam Style\"[31]', 'Psy', '5.20', 'July 15, 2012', '[E]', '\"Learning Colors – Colorful Eggs on a Farm\"[36]', 'Miroshka TV', '5.13', 'February 27, 2018', '', '\"Dame Tu Cosita\"[37]', 'Ultra Records', '4.68', 'April 5, 2018', '', '\"Axel F\"[38]', 'Crazy Frog', '4.60', 'June 16, 2009', '', '\"Masha and the Bear – Recipe for Disaster\"[39]', 'Get Movies', '4.60', 'January 31, 2012', '', '\"Baa Baa Black Sheep\"[40]', 'Cocomelon - Nursery Rhymes', '4.10', 'June 25, 2018', '', '\"Lakdi Ki Kathi\"[41]', 'Jingle Toons', '4.07', 'June 14, 2018', '', '\"Sugar\"[42]', 'Maroon 5', '4.06', 'January 14, 2015', '', '\"Counting Stars\"[43]', 'OneRepublic', '4.03', 'May 31, 2013', '', '\"Roar\"[44]', 'Katy Perry', '4.01', 'September 5, 2013', '', '\"Waka Waka (This Time for Africa)\"[45]', 'Shakira', '3.96', 'June 4, 2010', '', '\"Shree Hanuman Chalisa\"[46]', 'T-Series Bhakti Sagar', '3.90', 'May 10, 2011', '', '\"Humpty the train on a fruits ride\"[47]', 'Kiddiestv Hindi - Nursery Rhymes & Kids Songs', '3.84', 'January 26, 2018', '', '\"Sorry\"[48]', 'Justin Bieber', '3.82', 'October 22, 2015', '', '\"Thinking Out Loud\"[49]', 'Ed Sheeran', '3.78', 'October 7, 2014', '', '\"Perfect\"[50]', 'Ed Sheeran', '3.75', 'November 9, 2017', '', '\"Dark Horse\"[51]', 'Katy Perry', '3.74', 'February 20, 2014', '', '\"Let Her Go\"[52]', 'Passenger', '3.68', 'July 25, 2012', '', '\"Faded\"[53]', 'Alan Walker', '3.65', 'December 3, 2015', '', '\"Girls Like You\"[54]', 'Maroon 5', '3.62', 'May 31, 2018', '', '\"Lean On\"[55]', 'Major Lazer Official', '3.62', 'March 22, 2015', '']\n"
     ]
    }
   ],
   "source": [
    "# Total row data in table \n",
    "tot_element=[]\n",
    "table_ele=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td')\n",
    "for i in table_ele:\n",
    "    tot_element.append(i.text)\n",
    "print(tot_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41aeaf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tot_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce701d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Baby Shark Dance\"[7]', '\"Despacito\"[10]', '\"Johny Johny Yes Papa\"[18]', '\"Bath Song\"[19]', '\"See You Again\"[20]', '\"Shape of You\"[25]', '\"Wheels on the Bus\"[28]', '\"Phonics Song with Two Words\"[29]', '\"Uptown Funk\"[30]', '\"Gangnam Style\"[31]', '\"Learning Colors – Colorful Eggs on a Farm\"[36]', '\"Dame Tu Cosita\"[37]', '\"Axel F\"[38]', '\"Masha and the Bear – Recipe for Disaster\"[39]', '\"Baa Baa Black Sheep\"[40]', '\"Lakdi Ki Kathi\"[41]', '\"Sugar\"[42]', '\"Counting Stars\"[43]', '\"Roar\"[44]', '\"Waka Waka (This Time for Africa)\"[45]', '\"Shree Hanuman Chalisa\"[46]', '\"Humpty the train on a fruits ride\"[47]', '\"Sorry\"[48]', '\"Thinking Out Loud\"[49]', '\"Perfect\"[50]', '\"Dark Horse\"[51]', '\"Let Her Go\"[52]', '\"Faded\"[53]', '\"Girls Like You\"[54]', '\"Lean On\"[55]']\n"
     ]
    }
   ],
   "source": [
    "# Name of the Video\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "name=[]\n",
    "try:\n",
    "    namee=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "    for i in namee:\n",
    "        if i.text is not None :\n",
    "            name.append(i.text)             \n",
    "except:\n",
    "    name.append(\"$\")           \n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6b51a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3da31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Luis Fonsi', \"LooLoo Kids - Nursery Rhymes and Children's Songs\", 'Cocomelon - Nursery Rhymes', 'Wiz Khalifa', 'Ed Sheeran', 'Cocomelon - Nursery Rhymes', 'ChuChu TV Nursery Rhymes & Kids Songs', 'Mark Ronson', 'Psy', 'Miroshka TV', 'Ultra Records', 'Crazy Frog', 'Get Movies', 'Cocomelon - Nursery Rhymes', 'Jingle Toons', 'Maroon 5', 'OneRepublic', 'Katy Perry', 'Shakira', 'T-Series Bhakti Sagar', 'Kiddiestv Hindi - Nursery Rhymes & Kids Songs', 'Justin Bieber', 'Ed Sheeran', 'Ed Sheeran', 'Katy Perry', 'Passenger', 'Alan Walker', 'Maroon 5', 'Major Lazer Official']\n"
     ]
    }
   ],
   "source": [
    "artist=[]\n",
    "try:\n",
    "    namee=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[2]')\n",
    "    for i in namee:\n",
    "        if i.text is not None :\n",
    "            artist.append(i.text)             \n",
    "except:\n",
    "    artist.append(\"$\")           \n",
    "print(artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b8c705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd061f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14.66', '8.47', '6.92', '6.75', '6.30', '6.28', '6.22', '5.84', '5.25', '5.20', '5.13', '4.68', '4.60', '4.60', '4.10', '4.07', '4.06', '4.03', '4.01', '3.96', '3.90', '3.84', '3.82', '3.78', '3.75', '3.74', '3.68', '3.65', '3.62', '3.62']\n"
     ]
    }
   ],
   "source": [
    "views=[]\n",
    "try:\n",
    "    namee=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[3]')\n",
    "    for i in namee:\n",
    "        if i.text is not None :\n",
    "            views.append(i.text)             \n",
    "except:\n",
    "    views.append(\"$\")           \n",
    "print(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09392e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278c1e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'May 2, 2018', 'April 6, 2015', 'January 30, 2017', 'May 24, 2018', 'March 6, 2014', 'November 19, 2014', 'July 15, 2012', 'February 27, 2018', 'April 5, 2018', 'June 16, 2009', 'January 31, 2012', 'June 25, 2018', 'June 14, 2018', 'January 14, 2015', 'May 31, 2013', 'September 5, 2013', 'June 4, 2010', 'May 10, 2011', 'January 26, 2018', 'October 22, 2015', 'October 7, 2014', 'November 9, 2017', 'February 20, 2014', 'July 25, 2012', 'December 3, 2015', 'May 31, 2018', 'March 22, 2015']\n"
     ]
    }
   ],
   "source": [
    "date=[]\n",
    "try:\n",
    "    namee=driver.find_elements(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers sort-under col3center col4right jquery-tablesorter\"]/tbody/tr/td[4]')\n",
    "    for i in namee:\n",
    "        if i.text is not None :\n",
    "            date.append(i.text)             \n",
    "except:\n",
    "    date.append(\"$\")           \n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbd08615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f808e853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vedio Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Views</th>\n",
       "      <th>Update Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[7]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>14.66</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Despacito\"[10]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.47</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Johny Johny Yes Papa\"[18]</td>\n",
       "      <td>LooLoo Kids - Nursery Rhymes and Children's Songs</td>\n",
       "      <td>6.92</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Bath Song\"[19]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.75</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"See You Again\"[20]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>6.30</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Shape of You\"[25]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.28</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Wheels on the Bus\"[28]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>6.22</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Phonics Song with Two Words\"[29]</td>\n",
       "      <td>ChuChu TV Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>5.84</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Uptown Funk\"[30]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>5.25</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Gangnam Style\"[31]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>5.20</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[36]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>5.13</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Dame Tu Cosita\"[37]</td>\n",
       "      <td>Ultra Records</td>\n",
       "      <td>4.68</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Axel F\"[38]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>4.60</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[39]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.60</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Baa Baa Black Sheep\"[40]</td>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>4.10</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Lakdi Ki Kathi\"[41]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>4.07</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\"Sugar\"[42]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>4.06</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"Counting Stars\"[43]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>4.03</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Roar\"[44]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>4.01</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[45]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.96</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"Shree Hanuman Chalisa\"[46]</td>\n",
       "      <td>T-Series Bhakti Sagar</td>\n",
       "      <td>3.90</td>\n",
       "      <td>May 10, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>\"Humpty the train on a fruits ride\"[47]</td>\n",
       "      <td>Kiddiestv Hindi - Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.84</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Sorry\"[48]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.82</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Thinking Out Loud\"[49]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.78</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>\"Perfect\"[50]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.75</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>\"Dark Horse\"[51]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.74</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Let Her Go\"[52]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.68</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>\"Faded\"[53]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.65</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>\"Girls Like You\"[54]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.62</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Lean On\"[55]</td>\n",
       "      <td>Major Lazer Official</td>\n",
       "      <td>3.62</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Vedio Name  \\\n",
       "0                             \"Baby Shark Dance\"[7]   \n",
       "1                                   \"Despacito\"[10]   \n",
       "2                        \"Johny Johny Yes Papa\"[18]   \n",
       "3                                   \"Bath Song\"[19]   \n",
       "4                               \"See You Again\"[20]   \n",
       "5                                \"Shape of You\"[25]   \n",
       "6                           \"Wheels on the Bus\"[28]   \n",
       "7                 \"Phonics Song with Two Words\"[29]   \n",
       "8                                 \"Uptown Funk\"[30]   \n",
       "9                               \"Gangnam Style\"[31]   \n",
       "10  \"Learning Colors – Colorful Eggs on a Farm\"[36]   \n",
       "11                             \"Dame Tu Cosita\"[37]   \n",
       "12                                     \"Axel F\"[38]   \n",
       "13   \"Masha and the Bear – Recipe for Disaster\"[39]   \n",
       "14                        \"Baa Baa Black Sheep\"[40]   \n",
       "15                             \"Lakdi Ki Kathi\"[41]   \n",
       "16                                      \"Sugar\"[42]   \n",
       "17                             \"Counting Stars\"[43]   \n",
       "18                                       \"Roar\"[44]   \n",
       "19           \"Waka Waka (This Time for Africa)\"[45]   \n",
       "20                      \"Shree Hanuman Chalisa\"[46]   \n",
       "21          \"Humpty the train on a fruits ride\"[47]   \n",
       "22                                      \"Sorry\"[48]   \n",
       "23                          \"Thinking Out Loud\"[49]   \n",
       "24                                    \"Perfect\"[50]   \n",
       "25                                 \"Dark Horse\"[51]   \n",
       "26                                 \"Let Her Go\"[52]   \n",
       "27                                      \"Faded\"[53]   \n",
       "28                             \"Girls Like You\"[54]   \n",
       "29                                    \"Lean On\"[55]   \n",
       "\n",
       "                                          Artist Name  Views  \\\n",
       "0         Pinkfong Baby Shark - Kids' Songs & Stories  14.66   \n",
       "1                                          Luis Fonsi   8.47   \n",
       "2   LooLoo Kids - Nursery Rhymes and Children's Songs   6.92   \n",
       "3                          Cocomelon - Nursery Rhymes   6.75   \n",
       "4                                         Wiz Khalifa   6.30   \n",
       "5                                          Ed Sheeran   6.28   \n",
       "6                          Cocomelon - Nursery Rhymes   6.22   \n",
       "7               ChuChu TV Nursery Rhymes & Kids Songs   5.84   \n",
       "8                                         Mark Ronson   5.25   \n",
       "9                                                 Psy   5.20   \n",
       "10                                        Miroshka TV   5.13   \n",
       "11                                      Ultra Records   4.68   \n",
       "12                                         Crazy Frog   4.60   \n",
       "13                                         Get Movies   4.60   \n",
       "14                         Cocomelon - Nursery Rhymes   4.10   \n",
       "15                                       Jingle Toons   4.07   \n",
       "16                                           Maroon 5   4.06   \n",
       "17                                        OneRepublic   4.03   \n",
       "18                                         Katy Perry   4.01   \n",
       "19                                            Shakira   3.96   \n",
       "20                              T-Series Bhakti Sagar   3.90   \n",
       "21      Kiddiestv Hindi - Nursery Rhymes & Kids Songs   3.84   \n",
       "22                                      Justin Bieber   3.82   \n",
       "23                                         Ed Sheeran   3.78   \n",
       "24                                         Ed Sheeran   3.75   \n",
       "25                                         Katy Perry   3.74   \n",
       "26                                          Passenger   3.68   \n",
       "27                                        Alan Walker   3.65   \n",
       "28                                           Maroon 5   3.62   \n",
       "29                               Major Lazer Official   3.62   \n",
       "\n",
       "          Update Date  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4       April 6, 2015  \n",
       "5    January 30, 2017  \n",
       "6        May 24, 2018  \n",
       "7       March 6, 2014  \n",
       "8   November 19, 2014  \n",
       "9       July 15, 2012  \n",
       "10  February 27, 2018  \n",
       "11      April 5, 2018  \n",
       "12      June 16, 2009  \n",
       "13   January 31, 2012  \n",
       "14      June 25, 2018  \n",
       "15      June 14, 2018  \n",
       "16   January 14, 2015  \n",
       "17       May 31, 2013  \n",
       "18  September 5, 2013  \n",
       "19       June 4, 2010  \n",
       "20       May 10, 2011  \n",
       "21   January 26, 2018  \n",
       "22   October 22, 2015  \n",
       "23    October 7, 2014  \n",
       "24   November 9, 2017  \n",
       "25  February 20, 2014  \n",
       "26      July 25, 2012  \n",
       "27   December 3, 2015  \n",
       "28       May 31, 2018  \n",
       "29     March 22, 2015  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Vedio Name':name,'Artist Name':artist,'Views':views,'Update Date':date})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "# Url = https://www.bcci.tv/.\n",
    "# You need to find following details:\n",
    "# A) Series\n",
    "# B) Place\n",
    "# C) Date\n",
    "# D) Time\n",
    "# Note: - From bcci.tv home page you have reach to the international fixture page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfaab3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3528403",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.bcci.tv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f350da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"imw-international-men\"]/a[2]')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f53d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"fixtures\"]/div[1]/div[3]/div[2]')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a864f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"fixtures\"]/div[2]/div[1]/div[2]')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50d0325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"fixtures\"]/div[3]/div[2]/div/button')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69133ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India Tour Of Zimbabwe T20I Series 2024', 'India Tour Of Zimbabwe T20I Series 2024', 'India Tour Of Zimbabwe T20I Series 2024', 'India Tour Of Zimbabwe T20I Series 2024', 'India Tour Of Zimbabwe T20I Series 2024', 'Bangladesh Tour Of India Test Series 2024', 'Bangladesh Tour Of India Test Series 2024', 'Bangladesh Tour Of India T20 Series 2024', 'Bangladesh Tour Of India T20 Series 2024', 'Bangladesh Tour Of India T20 Series 2024', 'New Zealand Tour Of India Test Series 2024', 'New Zealand Tour Of India Test Series 2024', 'New Zealand Tour Of India Test Series 2024'] 13\n"
     ]
    }
   ],
   "source": [
    "series=[]\n",
    "try:    \n",
    "    ser=driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "    for i in ser:\n",
    "        if i.text is not None:\n",
    "            series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    series.append('-')\n",
    "print(series,len(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2164ccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harare Sports Club,', 'Harare Sports Club,', 'Harare Sports Club,', 'Harare Sports Club,', 'Harare Sports Club,', 'MA Chidambaram Stadium,', 'Green Park,', 'Himachal Pradesh Cricket Association Stadium,', 'Arun Jaitley Stadium,', 'Rajiv Gandhi International Stadium,', 'M Chinnaswamy Stadium,', 'Maharashtra Cricket Association Stadium,', 'Wankhede Stadium,'] 13\n"
     ]
    }
   ],
   "source": [
    "statium=[]\n",
    "try:    \n",
    "    sta=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]')\n",
    "    for i in sta:\n",
    "        if i.text is not None:\n",
    "            statium.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    statium.append('-')\n",
    "print(statium,len(statium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ed884d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harare', 'Harare', 'Harare', 'Harare', 'Harare', 'Chennai', 'Kanpur', 'Dharamsala', 'Delhi', 'Hyderabad', 'Bengaluru', 'Pune', 'Mumbai'] 13\n"
     ]
    }
   ],
   "source": [
    "country=[]\n",
    "try:    \n",
    "    coun=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]')\n",
    "    for i in coun:\n",
    "        if i.text is not None:\n",
    "            country.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    country.append('-')\n",
    "print(country,len(country)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "234ae5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6 Jul', '7 Jul', '10 Jul', '13 Jul', '14 Jul', '19 Sep', '27 Sep', '6 Oct', '9 Oct', '12 Oct', '16 Oct', '24 Oct', '1 Nov'] 13\n"
     ]
    }
   ],
   "source": [
    "date=[]\n",
    "try:    \n",
    "    dt=driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "    for i in dt:\n",
    "        if i.text is not None:\n",
    "            date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    date.append('-')\n",
    "print(date,len(date)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b2eaa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16:30 IST', '16:30 IST', '16:30 IST', '16:30 IST', '16:30 IST', '9:30', '9:30', '19:00 IST', '19:00 IST', '19:00 IST', '9:30 IST', '9:30 IST', '9:30 IST'] 13\n"
     ]
    }
   ],
   "source": [
    "time=[]\n",
    "\n",
    "try:    \n",
    "    tm=driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "    for i in tm:\n",
    "        if i.text is not None:\n",
    "            time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    time.append('-')\n",
    "print(time,len(time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c500c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series</th>\n",
       "      <th>Venue</th>\n",
       "      <th>country</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India Tour Of Zimbabwe T20I Series 2024</td>\n",
       "      <td>Harare Sports Club,</td>\n",
       "      <td>Harare</td>\n",
       "      <td>6 Jul</td>\n",
       "      <td>16:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India Tour Of Zimbabwe T20I Series 2024</td>\n",
       "      <td>Harare Sports Club,</td>\n",
       "      <td>Harare</td>\n",
       "      <td>7 Jul</td>\n",
       "      <td>16:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India Tour Of Zimbabwe T20I Series 2024</td>\n",
       "      <td>Harare Sports Club,</td>\n",
       "      <td>Harare</td>\n",
       "      <td>10 Jul</td>\n",
       "      <td>16:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India Tour Of Zimbabwe T20I Series 2024</td>\n",
       "      <td>Harare Sports Club,</td>\n",
       "      <td>Harare</td>\n",
       "      <td>13 Jul</td>\n",
       "      <td>16:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India Tour Of Zimbabwe T20I Series 2024</td>\n",
       "      <td>Harare Sports Club,</td>\n",
       "      <td>Harare</td>\n",
       "      <td>14 Jul</td>\n",
       "      <td>16:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bangladesh Tour Of India Test Series 2024</td>\n",
       "      <td>MA Chidambaram Stadium,</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>19 Sep</td>\n",
       "      <td>9:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh Tour Of India Test Series 2024</td>\n",
       "      <td>Green Park,</td>\n",
       "      <td>Kanpur</td>\n",
       "      <td>27 Sep</td>\n",
       "      <td>9:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh Tour Of India T20 Series 2024</td>\n",
       "      <td>Himachal Pradesh Cricket Association Stadium,</td>\n",
       "      <td>Dharamsala</td>\n",
       "      <td>6 Oct</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bangladesh Tour Of India T20 Series 2024</td>\n",
       "      <td>Arun Jaitley Stadium,</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>9 Oct</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bangladesh Tour Of India T20 Series 2024</td>\n",
       "      <td>Rajiv Gandhi International Stadium,</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>12 Oct</td>\n",
       "      <td>19:00 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New Zealand Tour Of India Test Series 2024</td>\n",
       "      <td>M Chinnaswamy Stadium,</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>16 Oct</td>\n",
       "      <td>9:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>New Zealand Tour Of India Test Series 2024</td>\n",
       "      <td>Maharashtra Cricket Association Stadium,</td>\n",
       "      <td>Pune</td>\n",
       "      <td>24 Oct</td>\n",
       "      <td>9:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>New Zealand Tour Of India Test Series 2024</td>\n",
       "      <td>Wankhede Stadium,</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>1 Nov</td>\n",
       "      <td>9:30 IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Series  \\\n",
       "0      India Tour Of Zimbabwe T20I Series 2024   \n",
       "1      India Tour Of Zimbabwe T20I Series 2024   \n",
       "2      India Tour Of Zimbabwe T20I Series 2024   \n",
       "3      India Tour Of Zimbabwe T20I Series 2024   \n",
       "4      India Tour Of Zimbabwe T20I Series 2024   \n",
       "5    Bangladesh Tour Of India Test Series 2024   \n",
       "6    Bangladesh Tour Of India Test Series 2024   \n",
       "7     Bangladesh Tour Of India T20 Series 2024   \n",
       "8     Bangladesh Tour Of India T20 Series 2024   \n",
       "9     Bangladesh Tour Of India T20 Series 2024   \n",
       "10  New Zealand Tour Of India Test Series 2024   \n",
       "11  New Zealand Tour Of India Test Series 2024   \n",
       "12  New Zealand Tour Of India Test Series 2024   \n",
       "\n",
       "                                            Venue     country    Date  \\\n",
       "0                             Harare Sports Club,      Harare   6 Jul   \n",
       "1                             Harare Sports Club,      Harare   7 Jul   \n",
       "2                             Harare Sports Club,      Harare  10 Jul   \n",
       "3                             Harare Sports Club,      Harare  13 Jul   \n",
       "4                             Harare Sports Club,      Harare  14 Jul   \n",
       "5                         MA Chidambaram Stadium,     Chennai  19 Sep   \n",
       "6                                     Green Park,      Kanpur  27 Sep   \n",
       "7   Himachal Pradesh Cricket Association Stadium,  Dharamsala   6 Oct   \n",
       "8                           Arun Jaitley Stadium,       Delhi   9 Oct   \n",
       "9             Rajiv Gandhi International Stadium,   Hyderabad  12 Oct   \n",
       "10                         M Chinnaswamy Stadium,   Bengaluru  16 Oct   \n",
       "11       Maharashtra Cricket Association Stadium,        Pune  24 Oct   \n",
       "12                              Wankhede Stadium,      Mumbai   1 Nov   \n",
       "\n",
       "         Time  \n",
       "0   16:30 IST  \n",
       "1   16:30 IST  \n",
       "2   16:30 IST  \n",
       "3   16:30 IST  \n",
       "4   16:30 IST  \n",
       "5        9:30  \n",
       "6        9:30  \n",
       "7   19:00 IST  \n",
       "8   19:00 IST  \n",
       "9   19:00 IST  \n",
       "10   9:30 IST  \n",
       "11   9:30 IST  \n",
       "12   9:30 IST  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Series':series,'Venue':statium,'country':country,'Date':date,'Time':time})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "# Url = http://statisticstimes.com/\n",
    "# You have to find following details: A) Rank\n",
    "# B) State\n",
    "# C) GSDP(18-19)- at current prices\n",
    "# D) GSDP(19-20)- at current prices\n",
    "# E) Share(18-19)\n",
    "# F) GDP($ billion)\n",
    "# Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f63a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aefcdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://statisticstimes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46cb970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86fc1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/div/a[3]')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "183acc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfdbd687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', ''] 68\n"
     ]
    }
   ],
   "source": [
    "rank=[]\n",
    "try:    \n",
    "    rk=driver.find_elements(By.XPATH,'//td[@class=\"data1\"]')\n",
    "    for i in rk:\n",
    "        if i.text is not None:\n",
    "            rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('-')\n",
    "print(rank,len(rank)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13972261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maharashtra', 'Tamil Nadu', 'Karnataka', 'Uttar Pradesh', 'Gujarat', 'West Bengal', 'Rajasthan', 'Andhra Pradesh', 'Telangana', 'Madhya Pradesh', 'Kerala', 'Delhi', 'Haryana', 'Odisha', 'Bihar', 'Punjab', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Arunachal Pradesh', 'Nagaland', 'Mizoram', 'Andaman & Nicobar Islands', 'India', 'Maharashtra', 'Tamil Nadu', 'Karnataka', 'Uttar Pradesh', 'Gujarat', 'West Bengal', 'Rajasthan', 'Andhra Pradesh', 'Telangana', 'Madhya Pradesh', 'Kerala', 'Delhi', 'Haryana', 'Bihar', 'Odisha', 'Punjab', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Manipur', 'Sikkim', 'Arunachal Pradesh', 'Nagaland', 'Mizoram', 'Andaman & Nicobar Islands', 'India'] 68\n"
     ]
    }
   ],
   "source": [
    "state=[]\n",
    "try:    \n",
    "    st=driver.find_elements(By.XPATH,'//td[@class=\"name\"]')\n",
    "    for i in st:\n",
    "        if i.text is not None:\n",
    "            state.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    state.append('-')\n",
    "print(state,len(state)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6386a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3,108,022', '2,071,286', '1,978,094', '1,975,595', '1,928,683', '1,329,238', '1,193,489', '1,148,471', '1,124,204', '1,092,964', '934,542', '881,336', '868,905', '662,886', '650,302', '617,192', '411,454', '410,525', '358,863', '267,143', '193,352', '172,162', '84,266', '62,550', '46,096', '43,810', '38,785', '37,557', '36,594', '34,775', '31,038', '27,824', '10,371'] 33\n"
     ]
    }
   ],
   "source": [
    "gsdp18_19=[]\n",
    "try:    \n",
    "    rk=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "    for i in rk:\n",
    "        if i.text is not None:\n",
    "            gsdp18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp18_19.append('$')\n",
    "print(gsdp18_19,len(gsdp18_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca19ed19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '2,364,514', '2,269,995', '2,258,040', '2,230,609', '1,531,758', '1,365,849', '1,303,524', '1,308,034', '1,246,471', '1,046,188', '1,014,688', '984,055', '753,177', '751,396', '676,164', '493,167', '464,399', '393,722', '303,781', '224,226', '191,728', '93,672', '72,636', '54,285', '49,643', '42,697', '42,756', '-', '39,630', '35,643', '-', '-'] 33\n"
     ]
    }
   ],
   "source": [
    "gsdp19_20=[]\n",
    "try:    \n",
    "    gs=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "    for i in gs:\n",
    "        if i.text is not None:\n",
    "            gsdp19_20.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gsdp19_20.append('$')\n",
    "print(gsdp19_20,len(gsdp19_20)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "187dbb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['13.17%', '8.78%', '8.38%', '8.37%', '8.17%', '5.63%', '5.06%', '4.87%', '4.76%', '4.63%', '3.96%', '3.73%', '3.68%', '2.81%', '2.76%', '2.62%', '1.74%', '1.74%', '1.52%', '1.13%', '0.82%', '0.73%', '0.36%', '0.27%', '0.20%', '0.19%', '0.16%', '0.16%', '0.16%', '0.15%', '0.13%', '0.12%', '0.04%'] 33\n"
     ]
    }
   ],
   "source": [
    "share18_19=[]\n",
    "try:    \n",
    "    sh=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "    for i in sh:\n",
    "        if i.text is not None:\n",
    "            share18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    share18_19.append('$')\n",
    "print(share18_19,len(share18_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "733eb2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['414.928', '276.522', '264.080', '263.747', '257.484', '177.456', '159.334', '153.324', '150.084', '145.913', '124.764', '117.660', '116.001', '88.497', '86.817', '82.397', '54.930', '54.806', '47.909', '35.664', '25.813', '22.984', '11.250', '8.351', '6.154', '5.849', '5.178', '5.014', '4.885', '4.643', '4.144', '3.715', '1.385'] 33\n"
     ]
    }
   ],
   "source": [
    "gdp18_19=[]\n",
    "try:    \n",
    "    gd=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[7]')\n",
    "    for i in gd:\n",
    "        if i.text is not None:\n",
    "            gdp18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    gdp18_19.append('$')\n",
    "print(gdp18_19,len(gdp18_19)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1ce64e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP 18_19</th>\n",
       "      <th>GSDP 19_20</th>\n",
       "      <th>Share</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>3,108,022</td>\n",
       "      <td>-</td>\n",
       "      <td>13.17%</td>\n",
       "      <td>414.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>2,071,286</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>8.78%</td>\n",
       "      <td>276.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,978,094</td>\n",
       "      <td>2,269,995</td>\n",
       "      <td>8.38%</td>\n",
       "      <td>264.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,975,595</td>\n",
       "      <td>2,258,040</td>\n",
       "      <td>8.37%</td>\n",
       "      <td>263.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,928,683</td>\n",
       "      <td>2,230,609</td>\n",
       "      <td>8.17%</td>\n",
       "      <td>257.484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,329,238</td>\n",
       "      <td>1,531,758</td>\n",
       "      <td>5.63%</td>\n",
       "      <td>177.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>1,193,489</td>\n",
       "      <td>1,365,849</td>\n",
       "      <td>5.06%</td>\n",
       "      <td>159.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>1,148,471</td>\n",
       "      <td>1,303,524</td>\n",
       "      <td>4.87%</td>\n",
       "      <td>153.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>1,124,204</td>\n",
       "      <td>1,308,034</td>\n",
       "      <td>4.76%</td>\n",
       "      <td>150.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>1,092,964</td>\n",
       "      <td>1,246,471</td>\n",
       "      <td>4.63%</td>\n",
       "      <td>145.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>934,542</td>\n",
       "      <td>1,046,188</td>\n",
       "      <td>3.96%</td>\n",
       "      <td>124.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>881,336</td>\n",
       "      <td>1,014,688</td>\n",
       "      <td>3.73%</td>\n",
       "      <td>117.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>868,905</td>\n",
       "      <td>984,055</td>\n",
       "      <td>3.68%</td>\n",
       "      <td>116.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>662,886</td>\n",
       "      <td>753,177</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>88.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>650,302</td>\n",
       "      <td>751,396</td>\n",
       "      <td>2.76%</td>\n",
       "      <td>86.817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>617,192</td>\n",
       "      <td>676,164</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>82.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>411,454</td>\n",
       "      <td>493,167</td>\n",
       "      <td>1.74%</td>\n",
       "      <td>54.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>410,525</td>\n",
       "      <td>464,399</td>\n",
       "      <td>1.74%</td>\n",
       "      <td>54.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>358,863</td>\n",
       "      <td>393,722</td>\n",
       "      <td>1.52%</td>\n",
       "      <td>47.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>267,143</td>\n",
       "      <td>303,781</td>\n",
       "      <td>1.13%</td>\n",
       "      <td>35.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>193,352</td>\n",
       "      <td>224,226</td>\n",
       "      <td>0.82%</td>\n",
       "      <td>25.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>172,162</td>\n",
       "      <td>191,728</td>\n",
       "      <td>0.73%</td>\n",
       "      <td>22.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>84,266</td>\n",
       "      <td>93,672</td>\n",
       "      <td>0.36%</td>\n",
       "      <td>11.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>62,550</td>\n",
       "      <td>72,636</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>8.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>46,096</td>\n",
       "      <td>54,285</td>\n",
       "      <td>0.20%</td>\n",
       "      <td>6.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>43,810</td>\n",
       "      <td>49,643</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>38,785</td>\n",
       "      <td>42,697</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>5.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>37,557</td>\n",
       "      <td>42,756</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>5.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>36,594</td>\n",
       "      <td>-</td>\n",
       "      <td>0.16%</td>\n",
       "      <td>4.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>34,775</td>\n",
       "      <td>39,630</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>31,038</td>\n",
       "      <td>35,643</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>27,824</td>\n",
       "      <td>-</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>10,371</td>\n",
       "      <td>-</td>\n",
       "      <td>0.04%</td>\n",
       "      <td>1.385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP 18_19 GSDP 19_20   Share      GDP\n",
       "0     1                Maharashtra  3,108,022          -  13.17%  414.928\n",
       "1     2                 Tamil Nadu  2,071,286  2,364,514   8.78%  276.522\n",
       "2     3                  Karnataka  1,978,094  2,269,995   8.38%  264.080\n",
       "3     4              Uttar Pradesh  1,975,595  2,258,040   8.37%  263.747\n",
       "4     5                    Gujarat  1,928,683  2,230,609   8.17%  257.484\n",
       "5     6                West Bengal  1,329,238  1,531,758   5.63%  177.456\n",
       "6     7                  Rajasthan  1,193,489  1,365,849   5.06%  159.334\n",
       "7     8             Andhra Pradesh  1,148,471  1,303,524   4.87%  153.324\n",
       "8     9                  Telangana  1,124,204  1,308,034   4.76%  150.084\n",
       "9    10             Madhya Pradesh  1,092,964  1,246,471   4.63%  145.913\n",
       "10   11                     Kerala    934,542  1,046,188   3.96%  124.764\n",
       "11   12                      Delhi    881,336  1,014,688   3.73%  117.660\n",
       "12   13                    Haryana    868,905    984,055   3.68%  116.001\n",
       "13   14                     Odisha    662,886    753,177   2.81%   88.497\n",
       "14   15                      Bihar    650,302    751,396   2.76%   86.817\n",
       "15   16                     Punjab    617,192    676,164   2.62%   82.397\n",
       "16   17                      Assam    411,454    493,167   1.74%   54.930\n",
       "17   18               Chhattisgarh    410,525    464,399   1.74%   54.806\n",
       "18   19                  Jharkhand    358,863    393,722   1.52%   47.909\n",
       "19   20                Uttarakhand    267,143    303,781   1.13%   35.664\n",
       "20   21            Jammu & Kashmir    193,352    224,226   0.82%   25.813\n",
       "21   22           Himachal Pradesh    172,162    191,728   0.73%   22.984\n",
       "22   23                        Goa     84,266     93,672   0.36%   11.250\n",
       "23   24                    Tripura     62,550     72,636   0.27%    8.351\n",
       "24   25                 Chandigarh     46,096     54,285   0.20%    6.154\n",
       "25   26                 Puducherry     43,810     49,643   0.19%    5.849\n",
       "26   27                  Meghalaya     38,785     42,697   0.16%    5.178\n",
       "27   28                     Sikkim     37,557     42,756   0.16%    5.014\n",
       "28   29                    Manipur     36,594          -   0.16%    4.885\n",
       "29   30          Arunachal Pradesh     34,775     39,630   0.15%    4.643\n",
       "30   31                   Nagaland     31,038     35,643   0.13%    4.144\n",
       "31   32                    Mizoram     27,824          -   0.12%    3.715\n",
       "32   33  Andaman & Nicobar Islands     10,371          -   0.04%    1.385"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Rank':rank[:33],'State':state[:33],'GSDP 18_19':gsdp18_19,'GSDP 19_20':gsdp19_20,'Share':share18_19,'GDP':gdp18_19})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3730b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Scrape the details of trending repositories on Github.com.\n",
    "# Url = https://github.com/\n",
    "# You have to find the following details:\n",
    "# A) Repository title\n",
    "# B) Repository description\n",
    "# C) Contributors count\n",
    "# D) Language used\n",
    "# Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a60fa454",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f07bdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://github.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cc57089",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[4]/button')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "180a8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[4]/div/div[3]/ul/li[2]/a')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "476597bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goldmansachs / gs-quant', 'WerWolv / ImHex', 'microsoft / generative-ai-for-beginners', 'microsoft / semantic-kernel', 'neovim / neovim', 'starship / starship', 'Asabeneh / 30-Days-Of-Python', 'YaLTeR / niri', 'astral-sh / uv', 'yt-dlp / yt-dlp', 'pedroslopez / whatsapp-web.js', 'overleaf / overleaf', 'VinciGit00 / Scrapegraph-ai', 'tigerbeetle / tigerbeetle', 'EricLBuehler / mistral.rs', 'danielmiessler / fabric', 'cuixueshe / earthworm', 'XingangPan / DragGAN', 'hajimehoshi / ebiten'] 19\n"
     ]
    }
   ],
   "source": [
    "reposit=[]\n",
    "try:    \n",
    "    rep=driver.find_elements(By.XPATH,'//h2[@class=\"h3 lh-condensed\"]/a')\n",
    "    for i in rep:\n",
    "        if i.text is not None:\n",
    "            reposit.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    reposit.append('$')\n",
    "print(reposit,len(reposit)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "415d2f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python toolkit for quantitative finance', '🔍 A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM.', '18 Lessons, Get Started Building with Generative AI 🔗 https://microsoft.github.io/generative-ai-for-beginners/', 'Integrate cutting-edge LLM technology quickly and easily into your apps', 'Vim-fork focused on extensibility and usability', '☄🌌️ The minimal, blazing-fast, and infinitely customizable prompt for any shell!', '30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw', 'A scrollable-tiling Wayland compositor.', 'An extremely fast Python package installer and resolver, written in Rust.', 'A feature-rich command-line audio/video downloader', 'A WhatsApp client library for NodeJS that connects through the WhatsApp Web browser app', 'A web-based collaborative LaTeX editor', 'Python scraper based on AI', 'The distributed financial transactions database designed for mission critical safety and performance.', 'Blazingly fast LLM inference.', 'fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.', 'Learning English through the method of constructing sentences with conjunctions', 'Official Code for DragGAN (SIGGRAPH 2023)', 'Ebitengine - A dead simple 2D game engine for Go'] 19\n"
     ]
    }
   ],
   "source": [
    "descrip=[]\n",
    "try:    \n",
    "    des=driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "    for i in des:\n",
    "        if i.text is not None:\n",
    "            descrip.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    descrip.append('$')\n",
    "print(descrip,len(descrip)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c3d1af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['602', '1,652', '27,185', '2,939', '5,484', '1,825', '6,941', '72', '405', '5,962', '3,418', '1,367', '960', '439', '209', '1,832', '521', '3,409', '632'] 19\n"
     ]
    }
   ],
   "source": [
    "contri=[]\n",
    "try:    \n",
    "    con=driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/a[2]')\n",
    "    for i in con:\n",
    "        if i.text is not None:\n",
    "            contri.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    descrip.append('$')\n",
    "print(contri,len(contri)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfa0ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jupyter Notebook', 'C++', 'Jupyter Notebook', 'C#', 'Vim Script', 'Rust', 'Python', 'Rust', 'Rust', 'Python', 'JavaScript', 'JavaScript', 'Python', 'Zig', 'Rust', 'Python', 'TypeScript', 'Python', 'Go'] 19\n"
     ]
    }
   ],
   "source": [
    "lang=[]\n",
    "try:    \n",
    "    lg=driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/span/span[2]')\n",
    "    for i in lg:\n",
    "        if i.text is not None:\n",
    "            lang.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lang.append('$')\n",
    "print(lang,len(lang)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b82cbd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldmansachs / gs-quant</td>\n",
       "      <td>Python toolkit for quantitative finance</td>\n",
       "      <td>602</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WerWolv / ImHex</td>\n",
       "      <td>🔍 A Hex Editor for Reverse Engineers, Programm...</td>\n",
       "      <td>1,652</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft / generative-ai-for-beginners</td>\n",
       "      <td>18 Lessons, Get Started Building with Generati...</td>\n",
       "      <td>27,185</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft / semantic-kernel</td>\n",
       "      <td>Integrate cutting-edge LLM technology quickly ...</td>\n",
       "      <td>2,939</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neovim / neovim</td>\n",
       "      <td>Vim-fork focused on extensibility and usability</td>\n",
       "      <td>5,484</td>\n",
       "      <td>Vim Script</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>starship / starship</td>\n",
       "      <td>☄🌌️ The minimal, blazing-fast, and infinitely ...</td>\n",
       "      <td>1,825</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Asabeneh / 30-Days-Of-Python</td>\n",
       "      <td>30 days of Python programming challenge is a s...</td>\n",
       "      <td>6,941</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>YaLTeR / niri</td>\n",
       "      <td>A scrollable-tiling Wayland compositor.</td>\n",
       "      <td>72</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>astral-sh / uv</td>\n",
       "      <td>An extremely fast Python package installer and...</td>\n",
       "      <td>405</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yt-dlp / yt-dlp</td>\n",
       "      <td>A feature-rich command-line audio/video downlo...</td>\n",
       "      <td>5,962</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pedroslopez / whatsapp-web.js</td>\n",
       "      <td>A WhatsApp client library for NodeJS that conn...</td>\n",
       "      <td>3,418</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>overleaf / overleaf</td>\n",
       "      <td>A web-based collaborative LaTeX editor</td>\n",
       "      <td>1,367</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VinciGit00 / Scrapegraph-ai</td>\n",
       "      <td>Python scraper based on AI</td>\n",
       "      <td>960</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tigerbeetle / tigerbeetle</td>\n",
       "      <td>The distributed financial transactions databas...</td>\n",
       "      <td>439</td>\n",
       "      <td>Zig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EricLBuehler / mistral.rs</td>\n",
       "      <td>Blazingly fast LLM inference.</td>\n",
       "      <td>209</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>danielmiessler / fabric</td>\n",
       "      <td>fabric is an open-source framework for augment...</td>\n",
       "      <td>1,832</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cuixueshe / earthworm</td>\n",
       "      <td>Learning English through the method of constru...</td>\n",
       "      <td>521</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>XingangPan / DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td>3,409</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hajimehoshi / ebiten</td>\n",
       "      <td>Ebitengine - A dead simple 2D game engine for Go</td>\n",
       "      <td>632</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Repository Title  \\\n",
       "0                   goldmansachs / gs-quant   \n",
       "1                           WerWolv / ImHex   \n",
       "2   microsoft / generative-ai-for-beginners   \n",
       "3               microsoft / semantic-kernel   \n",
       "4                           neovim / neovim   \n",
       "5                       starship / starship   \n",
       "6              Asabeneh / 30-Days-Of-Python   \n",
       "7                             YaLTeR / niri   \n",
       "8                            astral-sh / uv   \n",
       "9                           yt-dlp / yt-dlp   \n",
       "10            pedroslopez / whatsapp-web.js   \n",
       "11                      overleaf / overleaf   \n",
       "12              VinciGit00 / Scrapegraph-ai   \n",
       "13                tigerbeetle / tigerbeetle   \n",
       "14                EricLBuehler / mistral.rs   \n",
       "15                  danielmiessler / fabric   \n",
       "16                    cuixueshe / earthworm   \n",
       "17                     XingangPan / DragGAN   \n",
       "18                     hajimehoshi / ebiten   \n",
       "\n",
       "                                          Description Contributors Count  \\\n",
       "0             Python toolkit for quantitative finance                602   \n",
       "1   🔍 A Hex Editor for Reverse Engineers, Programm...              1,652   \n",
       "2   18 Lessons, Get Started Building with Generati...             27,185   \n",
       "3   Integrate cutting-edge LLM technology quickly ...              2,939   \n",
       "4     Vim-fork focused on extensibility and usability              5,484   \n",
       "5   ☄🌌️ The minimal, blazing-fast, and infinitely ...              1,825   \n",
       "6   30 days of Python programming challenge is a s...              6,941   \n",
       "7             A scrollable-tiling Wayland compositor.                 72   \n",
       "8   An extremely fast Python package installer and...                405   \n",
       "9   A feature-rich command-line audio/video downlo...              5,962   \n",
       "10  A WhatsApp client library for NodeJS that conn...              3,418   \n",
       "11             A web-based collaborative LaTeX editor              1,367   \n",
       "12                         Python scraper based on AI                960   \n",
       "13  The distributed financial transactions databas...                439   \n",
       "14                      Blazingly fast LLM inference.                209   \n",
       "15  fabric is an open-source framework for augment...              1,832   \n",
       "16  Learning English through the method of constru...                521   \n",
       "17          Official Code for DragGAN (SIGGRAPH 2023)              3,409   \n",
       "18   Ebitengine - A dead simple 2D game engine for Go                632   \n",
       "\n",
       "            Language  \n",
       "0   Jupyter Notebook  \n",
       "1                C++  \n",
       "2   Jupyter Notebook  \n",
       "3                 C#  \n",
       "4         Vim Script  \n",
       "5               Rust  \n",
       "6             Python  \n",
       "7               Rust  \n",
       "8               Rust  \n",
       "9             Python  \n",
       "10        JavaScript  \n",
       "11        JavaScript  \n",
       "12            Python  \n",
       "13               Zig  \n",
       "14              Rust  \n",
       "15            Python  \n",
       "16        TypeScript  \n",
       "17            Python  \n",
       "18                Go  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Repository Title':reposit,'Description':descrip,'Contributors Count':contri,'Language':lang})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5806cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the following \n",
    "# details:\n",
    "# A) Song name\n",
    "# B) Artist name\n",
    "# C) Last week rank\n",
    "# D) Peak rank\n",
    "# E) Weeks on board\n",
    "# Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b041fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8b65b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https:/www.billboard.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd0ab710",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b576bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div/div[3]/div/nav/ul/li[1]/a')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02378e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please Please Please', 'I Had Some Help', 'A Bar Song (Tipsy)', 'Espresso', 'Million Dollar Baby', 'Not Like Us', 'Too Sweet', 'Beautiful Things', 'Lose Control', 'Birds Of A Feather', 'Pink Skies', 'Houdini', 'Fortnight', 'Like That', 'Cowgirls', 'Good Luck, Babe!', 'Saturn', 'Lunch', \"We Can't Be Friends (Wait For Your Love)\", 'Austin', 'Whiskey Whiskey', 'I Remember Everything', 'Feather', 'Miles On It', 'Wanna Be', 'Lovin On Me', 'High Road', 'Stick Season', 'BAND4BAND', 'I Am Not Okay', 'Gata Only', 'Yeah Glo!', \"Ain't No Love In Oklahoma\", 'Slow It Down', 'Remember Him That Way', 'Chihiro', 'End Of Beginning', 'Get It Sexyy', 'The Man He Sees In Me', 'Bulletproof', 'Greedy', 'Agora Hills', 'Where It Ends', 'Stargazing', 'I Can Do It With A Broken Heart', 'I Like The Way You Kiss Me', 'Devil Is A Lie', 'Whatever She Wants', 'Sweet Dreams', 'One Of Wun', 'Type Shit', 'Euphoria', 'Dirt Cheap', 'Red Wine Supernova', 'Hot To Go!', 'Wild Ones', 'Wildflower', 'Attitude', 'U My Everything', 'Illusion', 'Brother Stone', 'Nasty', 'We Ride', 'Down Bad', 'Si No Quieres No', \"Think I'm In Love With You\", \"Wind Up Missin' You\", 'Wine Into Whiskey', 'Close To You', \"L'amour De Ma Vie\", 'The Thin Grey Line', 'Scared To Start', 'Bandit', 'The Boy Is Mine', 'Halfway To Hell', 'The Door', \"Texas Hold 'Em\", 'Tore Up', 'Belong Together', 'Your Place', 'After Hours', '360', \"Who's Afraid Of Little Old Me?\", 'Blue', 'Beautiful As You', 'Burgundy', 'Hell N Back', 'Guilty As Sin?', 'Enough (Miami)', 'Pink Pony Club', 'Thorns', 'Ice Age', 'Take Her Home', 'Let Your Boys Be Country', 'Front Door Famous', 'Spin You Around (1/24)', 'Get In With Me', 'Kryptonite', 'Carnival', 'Chevrolet'] 100\n"
     ]
    }
   ],
   "source": [
    "song=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/h3')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            song.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lang.append('$')\n",
    "print(song,len(song)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67ee770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sabrina Carpenter', 'Post Malone Featuring Morgan Wallen', 'Shaboozey', 'Sabrina Carpenter', 'Tommy Richman', 'Kendrick Lamar', 'Hozier', 'Benson Boone', 'Teddy Swims', 'Billie Eilish', 'Zach Bryan', 'Eminem', 'Taylor Swift Featuring Post Malone', 'Future, Metro Boomin & Kendrick Lamar', 'Morgan Wallen Featuring ERNEST', 'Chappell Roan', 'SZA', 'Billie Eilish', 'Ariana Grande', 'Dasha', 'Moneybagg Yo Featuring Morgan Wallen', 'Zach Bryan Featuring Kacey Musgraves', 'Sabrina Carpenter', 'Marshmello & Kane Brown', 'GloRilla & Megan Thee Stallion', 'Jack Harlow', 'Koe Wetzel & Jessie Murph', 'Noah Kahan', 'Central Cee & Lil Baby', 'Jelly Roll', 'FloyyMenor X Cris Mj', 'GloRilla', 'Luke Combs', 'Benson Boone', 'Luke Combs', 'Billie Eilish', 'Djo', 'Sexyy Red', 'Luke Combs', 'Nate Smith', 'Tate McRae', 'Doja Cat', 'Bailey Zimmerman', 'Myles Smith', 'Taylor Swift', 'Artemas', 'Tommy Richman', 'Bryson Tiller', 'Koe Wetzel', 'Gunna', 'Future, Metro Boomin, Travis Scott & Playboi Carti', 'Kendrick Lamar', 'Cody Johnson', 'Chappell Roan', 'Chappell Roan', 'Jessie Murph & Jelly Roll', 'Billie Eilish', 'Don Toliver Featuring Charlie Wilson & Cash Cobain', 'Sexyy Red & Drake', 'Dua Lipa', 'Don Toliver Featuring Kodak Black', 'Tinashe', 'Bryan Martin', 'Taylor Swift', 'Luis R Conriquez x Neton Vega', 'Chris Stapleton', 'Tucker Wetmore', 'Tucker Wetmore', 'Gracie Abrams', 'Billie Eilish', '$uicideboy$', 'Michael Marcagi', 'Don Toliver', 'Ariana Grande', 'Jelly Roll', 'Teddy Swims', 'Beyonce', 'Don Toliver', 'Mark Ambor', 'Ashley Cooke', 'Kehlani', 'Charli XCX', 'Taylor Swift', 'Billie Eilish', 'Thomas Rhett', '$uicideboy$', 'Bakar Featuring Summer Walker', 'Taylor Swift', 'Cardi B', 'Chappell Roan', '$uicideboy$', 'Don Toliver Featuring Travis Scott', 'Kenny Chesney', 'Jason Aldean', 'Luke Combs', 'Morgan Wallen', 'BossMan DLow', 'Don Toliver', '¥$: Ye & Ty Dolla $ign Featuring Rich The Kid & Playboi Carti', 'Dustin Lynch Featuring Jelly Roll'] 100\n"
     ]
    }
   ],
   "source": [
    "artist=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[1]/span')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lang.append('$')\n",
    "print(artist,len(artist)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "938f1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1', '4', '3', '5', '6', '7', '11', '10', '9', '12', '8', '13', '15', '17', '21', '16', '14', '18', '19', '-', '24', '22', '28', '20', '26', '29', '27', '23', '-', '31', '32', '30', '36', '-', '25', '34', '33', '58', '37', '38', '40', '46', '45', '44', '42', '-', '48', '35', '39', '43', '47', '56', '67', '68', '55', '41', '-', '53', '59', '-', '69', '62', '54', '57', '50', '63', '78', '49', '51', '-', '65', '-', '64', '52', '94', '74', '-', '76', '87', '84', '73', '72', '66', '96', '-', '83', '80', '75', '-', '-', '-', '71', '-', '-', '95', '88', '-', '82', '-'] 100\n"
     ]
    }
   ],
   "source": [
    "lrank=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[4]/span')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            lrank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    lang.append('$')\n",
    "print(lrank,len(lrank)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87c0f6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1', '3', '3', '2', '1', '1', '2', '1', '9', '6', '2', '1', '1', '15', '16', '6', '5', '1', '19', '21', '1', '21', '21', '11', '1', '27', '9', '18', '30', '27', '28', '23', '34', '35', '12', '11', '20', '39', '37', '3', '7', '32', '41', '3', '12', '47', '19', '35', '26', '2', '3', '53', '54', '55', '35', '17', '58', '44', '43', '61', '62', '61', '2', '53', '49', '63', '68', '49', '22', '71', '54', '38', '16', '48', '76', '1', '78', '74', '80', '81', '73', '9', '25', '85', '86', '53', '10', '9', '90', '91', '92', '71', '94', '95', '24', '49', '98', '1', '100'] 100\n"
     ]
    }
   ],
   "source": [
    "prank=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[5]/span')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            prank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    prank.append('$')\n",
    "print(prank,len(prank)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18928a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '6', '10', '10', '8', '7', '13', '22', '45', '5', '4', '3', '9', '13', '27', '11', '17', '5', '15', '15', '1', '43', '29', '7', '11', '32', '2', '38', '4', '1', '14', '19', '5', '13', '1', '5', '18', '14', '2', '11', '40', '39', '25', '6', '9', '13', '1', '18', '5', '6', '13', '8', '9', '3', '3', '37', '5', '2', '4', '10', '1', '3', '11', '9', '7', '7', '12', '13', '2', '5', '1', '18', '17', '7', '11', '3', '19', '1', '8', '4', '4', '2', '9', '5', '2', '1', '11', '9', '14', '1', '1', '1', '6', '2', '1', '20', '20', '1', '19', '1'] 100\n"
     ]
    }
   ],
   "source": [
    "wboard=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]/ul/li[6]/span')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            wboard.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    wboard.append('$')\n",
    "print(wboard,len(wboard)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5000ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please Please Please</td>\n",
       "      <td>Sabrina Carpenter</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I Had Some Help</td>\n",
       "      <td>Post Malone Featuring Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bar Song (Tipsy)</td>\n",
       "      <td>Shaboozey</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Espresso</td>\n",
       "      <td>Sabrina Carpenter</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Million Dollar Baby</td>\n",
       "      <td>Tommy Richman</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Spin You Around (1/24)</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>95</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Get In With Me</td>\n",
       "      <td>BossMan DLow</td>\n",
       "      <td>88</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Kryptonite</td>\n",
       "      <td>Don Toliver</td>\n",
       "      <td>-</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Carnival</td>\n",
       "      <td>¥$: Ye &amp; Ty Dolla $ign Featuring Rich The Kid ...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Dustin Lynch Featuring Jelly Roll</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Song Name                                        Artist Name  \\\n",
       "0     Please Please Please                                  Sabrina Carpenter   \n",
       "1          I Had Some Help                Post Malone Featuring Morgan Wallen   \n",
       "2       A Bar Song (Tipsy)                                          Shaboozey   \n",
       "3                 Espresso                                  Sabrina Carpenter   \n",
       "4      Million Dollar Baby                                      Tommy Richman   \n",
       "..                     ...                                                ...   \n",
       "95  Spin You Around (1/24)                                      Morgan Wallen   \n",
       "96          Get In With Me                                       BossMan DLow   \n",
       "97              Kryptonite                                        Don Toliver   \n",
       "98                Carnival  ¥$: Ye & Ty Dolla $ign Featuring Rich The Kid ...   \n",
       "99               Chevrolet                  Dustin Lynch Featuring Jelly Roll   \n",
       "\n",
       "   Last Week Rank Peak Rank Weeks on Board  \n",
       "0               2         1              2  \n",
       "1               1         1              6  \n",
       "2               4         3             10  \n",
       "3               3         3             10  \n",
       "4               5         2              8  \n",
       "..            ...       ...            ...  \n",
       "95             95        24             20  \n",
       "96             88        49             20  \n",
       "97              -        98              1  \n",
       "98             82         1             19  \n",
       "99              -       100              1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Song Name':song,'Artist Name':artist,'Last Week Rank':lrank,'Peak Rank':prank,'Weeks on Board':wboard})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Scrape the details of Highest selling novels.\n",
    "# A) Book name\n",
    "# B) Author name\n",
    "# C) Volumes sold\n",
    "# D) Publisher\n",
    "# E) Genre\n",
    "# Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afd5d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e0347fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13cb9e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100'] 100\n"
     ]
    }
   ],
   "source": [
    "rank=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[1]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append('$')\n",
    "print(rank,len(rank)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09bc2238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"] 100\n"
     ]
    }
   ],
   "source": [
    "book=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[2]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            book.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    book.append('$')\n",
    "print(book,len(book)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92a63d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie'] 100\n"
     ]
    }
   ],
   "source": [
    "author=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[3]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            author.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    author.append('$')\n",
    "print(author,len(author)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a42f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095'] 100\n"
     ]
    }
   ],
   "source": [
    "volume=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[4]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            volume.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    volume.append('$')\n",
    "print(volume,len(volume)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3eb34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin'] 100\n"
     ]
    }
   ],
   "source": [
    "publish=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[5]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            publish.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    publish.append('$')\n",
    "print(publish,len(publish)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d19c194c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General'] 100\n"
     ]
    }
   ],
   "source": [
    "genre=[]\n",
    "try:    \n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"embed block\"]/table/tbody/tr/td[6]')\n",
    "    for i in name:\n",
    "        if i.text is not None:\n",
    "            genre.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    genre.append('$')\n",
    "print(genre,len(genre)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5408e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume Sales</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                          Book Name            Author  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume Sales        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Rank':rank,'Book Name':book,'Author':author,'Volume Sales':volume,'Publisher':publish,'Genre':genre})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049259a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error showing in this question URL\n",
    "\n",
    "\n",
    "# Q7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "# Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "# A) Name\n",
    "# B) Year span\n",
    "# C) Genre\n",
    "# D) Run time\n",
    "# E) Ratings\n",
    "# F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60144136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Details of Datasets from UCI machine learning repositories.\n",
    "# Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "# A) Dataset name\n",
    "# B) Data type\n",
    "# C) Task\n",
    "# D) Attribute type\n",
    "# E) No of instances\n",
    "# F) No of attribute G) Year\n",
    "# Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7cf8368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "084f67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://archive.ics.uci.edu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47335bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_1 = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "search_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "db076286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://archive.ics.uci.edu/dataset/53/iris',\n",
       " 'https://archive.ics.uci.edu/dataset/602/dry+bean+dataset',\n",
       " 'https://archive.ics.uci.edu/dataset/45/heart+disease',\n",
       " 'https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik',\n",
       " 'https://archive.ics.uci.edu/dataset/850/raisin',\n",
       " 'https://archive.ics.uci.edu/dataset/2/adult',\n",
       " 'https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic',\n",
       " 'https://archive.ics.uci.edu/dataset/109/wine',\n",
       " 'https://archive.ics.uci.edu/dataset/186/wine+quality',\n",
       " 'https://archive.ics.uci.edu/dataset/222/bank+marketing',\n",
       " 'https://archive.ics.uci.edu/dataset/34/diabetes',\n",
       " 'https://archive.ics.uci.edu/dataset/19/car+evaluation',\n",
       " 'https://archive.ics.uci.edu/dataset/320/student+performance',\n",
       " 'https://archive.ics.uci.edu/dataset/1/abalone',\n",
       " 'https://archive.ics.uci.edu/dataset/73/mushroom',\n",
       " 'https://archive.ics.uci.edu/dataset/352/online+retail',\n",
       " 'https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data',\n",
       " 'https://archive.ics.uci.edu/dataset/10/automobile',\n",
       " 'https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success',\n",
       " 'https://archive.ics.uci.edu/dataset/20/census+income',\n",
       " 'https://archive.ics.uci.edu/dataset/34/diabetes',\n",
       " 'https://archive.ics.uci.edu/dataset/19/car+evaluation',\n",
       " 'https://archive.ics.uci.edu/dataset/320/student+performance',\n",
       " 'https://archive.ics.uci.edu/dataset/1/abalone',\n",
       " 'https://archive.ics.uci.edu/dataset/73/mushroom',\n",
       " 'https://archive.ics.uci.edu/dataset/352/online+retail',\n",
       " 'https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data',\n",
       " 'https://archive.ics.uci.edu/dataset/10/automobile',\n",
       " 'https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success',\n",
       " 'https://archive.ics.uci.edu/dataset/20/census+income',\n",
       " 'https://archive.ics.uci.edu/dataset/502/online+retail+ii',\n",
       " 'https://archive.ics.uci.edu/dataset/228/sms+spam+collection',\n",
       " 'https://archive.ics.uci.edu/dataset/102/thyroid+disease',\n",
       " 'https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition',\n",
       " 'https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits',\n",
       " 'https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption',\n",
       " 'https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope',\n",
       " 'https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset',\n",
       " 'https://archive.ics.uci.edu/dataset/111/zoo',\n",
       " 'https://archive.ics.uci.edu/dataset/162/forest+fires',\n",
       " 'https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records',\n",
       " 'https://archive.ics.uci.edu/dataset/31/covertype',\n",
       " 'https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008',\n",
       " 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators',\n",
       " 'https://archive.ics.uci.edu/dataset/62/lung+cancer',\n",
       " 'https://archive.ics.uci.edu/dataset/60/liver+disorders',\n",
       " 'https://archive.ics.uci.edu/dataset/59/letter+recognition',\n",
       " 'https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set',\n",
       " 'https://archive.ics.uci.edu/dataset/52/ionosphere',\n",
       " 'https://archive.ics.uci.edu/dataset/29/computer+hardware',\n",
       " 'https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records',\n",
       " 'https://archive.ics.uci.edu/dataset/31/covertype',\n",
       " 'https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008',\n",
       " 'https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators',\n",
       " 'https://archive.ics.uci.edu/dataset/62/lung+cancer',\n",
       " 'https://archive.ics.uci.edu/dataset/60/liver+disorders',\n",
       " 'https://archive.ics.uci.edu/dataset/59/letter+recognition',\n",
       " 'https://archive.ics.uci.edu/dataset/477/real+estate+valuation+data+set',\n",
       " 'https://archive.ics.uci.edu/dataset/52/ionosphere',\n",
       " 'https://archive.ics.uci.edu/dataset/29/computer+hardware',\n",
       " 'https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength',\n",
       " 'https://archive.ics.uci.edu/dataset/174/parkinsons',\n",
       " 'https://archive.ics.uci.edu/dataset/110/yeast',\n",
       " 'https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame',\n",
       " 'https://archive.ics.uci.edu/dataset/555/apartment+for+rent+classified',\n",
       " 'https://archive.ics.uci.edu/dataset/50/image+segmentation',\n",
       " 'https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014',\n",
       " 'https://archive.ics.uci.edu/dataset/5/arrhythmia',\n",
       " 'https://archive.ics.uci.edu/dataset/76/nursery',\n",
       " 'https://archive.ics.uci.edu/dataset/113/twenty+newsgroups',\n",
       " 'https://archive.ics.uci.edu/dataset/327/phishing+websites',\n",
       " 'https://archive.ics.uci.edu/dataset/81/pen+based+recognition+of+handwritten+digits',\n",
       " 'https://archive.ics.uci.edu/dataset/143/statlog+australian+credit+approval',\n",
       " 'https://archive.ics.uci.edu/dataset/145/statlog+heart',\n",
       " 'https://archive.ics.uci.edu/dataset/332/online+news+popularity',\n",
       " 'https://archive.ics.uci.edu/dataset/121/eeg+database',\n",
       " 'https://archive.ics.uci.edu/dataset/267/banknote+authentication',\n",
       " 'https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction',\n",
       " 'https://archive.ics.uci.edu/dataset/12/balance+scale',\n",
       " 'https://archive.ics.uci.edu/dataset/16/breast+cancer+wisconsin+prognostic',\n",
       " 'https://archive.ics.uci.edu/dataset/33/dermatology',\n",
       " 'https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation',\n",
       " 'https://archive.ics.uci.edu/dataset/529/early+stage+diabetes+risk+prediction+dataset',\n",
       " 'https://archive.ics.uci.edu/dataset/90/soybean+large',\n",
       " 'https://archive.ics.uci.edu/dataset/183/communities+and+crime',\n",
       " 'https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant',\n",
       " 'https://archive.ics.uci.edu/dataset/597/productivity+prediction+of+garment+employees',\n",
       " 'https://archive.ics.uci.edu/dataset/936/national+poll+on+healthy+aging+(npha)',\n",
       " 'https://archive.ics.uci.edu/dataset/462/drug+review+dataset+drugs+com',\n",
       " 'https://archive.ics.uci.edu/dataset/89/solar+flare',\n",
       " 'https://archive.ics.uci.edu/dataset/39/ecoli',\n",
       " 'https://archive.ics.uci.edu/dataset/132/movie',\n",
       " 'https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences',\n",
       " 'https://archive.ics.uci.edu/dataset/280/higgs',\n",
       " 'https://archive.ics.uci.edu/dataset/137/reuters+21578+text+categorization+collection',\n",
       " 'https://archive.ics.uci.edu/dataset/373/drug+consumption+quantified',\n",
       " 'https://archive.ics.uci.edu/dataset/149/statlog+vehicle+silhouettes',\n",
       " 'https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite',\n",
       " 'https://archive.ics.uci.edu/dataset/125/insurance+company+benchmark+coil+2000',\n",
       " 'https://archive.ics.uci.edu/dataset/148/statlog+shuttle']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "url=[]\n",
    "start=0\n",
    "end=10\n",
    "for k in range(start,end):\n",
    "    u= driver.find_elements(By.XPATH,'//a[@class=\"link-hover link text-xl font-semibold\"]')\n",
    "    for i in u:\n",
    "        url.append(i.get_attribute('href'))\n",
    "    search = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]')\n",
    "    search.click()  \n",
    "    time.sleep(2)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1da575b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris', 'Dry Bean', 'Heart Disease', 'Rice (Cammeo and Osmancik)', 'Raisin', 'Adult', 'Breast Cancer Wisconsin (Diagnostic)', 'Wine', 'Wine Quality', 'Bank Marketing', 'Diabetes', 'Car Evaluation', 'Student Performance', 'Abalone', 'Mushroom', 'Online Retail', 'Statlog (German Credit Data)', 'Automobile', \"Predict Students' Dropout and Academic Success\", 'Census Income', 'Diabetes', 'Car Evaluation', 'Student Performance', 'Abalone', 'Mushroom', 'Online Retail', 'Statlog (German Credit Data)', 'Automobile', \"Predict Students' Dropout and Academic Success\", 'Census Income', 'Online Retail II', 'SMS Spam Collection', 'Thyroid Disease', 'Estimation of Obesity Levels Based On Eating Habits and Physical Condition', 'Optical Recognition of Handwritten Digits', 'Individual Household Electric Power Consumption', 'MAGIC Gamma Telescope', 'Online Shoppers Purchasing Intention Dataset', 'Zoo', 'Forest Fires', 'Heart Failure Clinical Records', 'Covertype', 'Diabetes 130-US Hospitals for Years 1999-2008', 'CDC Diabetes Health Indicators', 'Lung Cancer', 'Liver Disorders', 'Letter Recognition', 'Real Estate Valuation', 'Ionosphere', 'Computer Hardware', 'Heart Failure Clinical Records', 'Covertype', 'Diabetes 130-US Hospitals for Years 1999-2008', 'CDC Diabetes Health Indicators', 'Lung Cancer', 'Liver Disorders', 'Letter Recognition', 'Real Estate Valuation', 'Ionosphere', 'Computer Hardware', 'Concrete Compressive Strength', 'Parkinsons', 'Yeast', 'Tic-Tac-Toe Endgame', 'Apartment for Rent Classified', 'Image Segmentation', 'ElectricityLoadDiagrams20112014', 'Arrhythmia', 'Nursery', 'Twenty Newsgroups', 'Phishing Websites', 'Pen-Based Recognition of Handwritten Digits', 'Statlog (Australian Credit Approval)', 'Statlog (Heart)', 'Online News Popularity', 'EEG Database', 'Banknote Authentication', 'Appliances Energy Prediction', 'Balance Scale', 'Breast Cancer Wisconsin (Prognostic)', 'Dermatology', 'Higher Education Students Performance Evaluation', 'Early Stage Diabetes Risk Prediction', 'Soybean (Large)', 'Communities and Crime', 'Combined Cycle Power Plant', 'Productivity Prediction of Garment Employees', 'National Poll on Healthy Aging (NPHA)', 'Drug Reviews (Drugs.com)', 'Solar Flare', 'Movie', 'Sentiment Labelled Sentences', 'HIGGS', 'Reuters-21578 Text Categorization Collection', 'Drug Consumption (Quantified)', 'Statlog (Vehicle Silhouettes)', 'Statlog (Landsat Satellite)', 'Insurance Company Benchmark (COIL 2000)', 'Statlog (Shuttle)'] 99\n",
      "------------------\n",
      "['Real', 'Integer, Real', 'Categorical, Integer, Real', 'Real', 'Real, Integer', 'Categorical, Integer', 'Real', 'Integer, Real', 'Real', 'Categorical, Integer', 'Categorical, Integer', 'Categorical', 'Integer', 'Categorical, Integer, Real', 'Categorical', 'Integer, Real', 'Categorical, Integer', 'Categorical, Integer, Real', 'Real, Categorical, Integer', 'Categorical, Integer', 'Categorical, Integer', 'Categorical', 'Integer', 'Categorical, Integer, Real', 'Categorical', 'Integer, Real', 'Categorical, Integer', 'Categorical, Integer, Real', 'Real, Categorical, Integer', 'Categorical, Integer', 'Integer, Real', 'Real', 'Categorical, Real', 'Integer', 'Integer', 'Real', 'Real', 'Integer, Real', 'Categorical, Integer', 'Real', 'Integer, Real', 'Categorical, Integer', 'Categorical, Integer', 'Categorical, Integer', 'Integer', 'Categorical, Integer, Real', 'Integer', 'Integer, Real', 'Integer, Real', 'Integer', 'Integer, Real', 'Categorical, Integer', 'Categorical, Integer', 'Categorical, Integer', 'Integer', 'Categorical, Integer, Real', 'Integer', 'Integer, Real', 'Integer, Real', 'Integer', 'Real', 'Real', 'Real', 'Categorical', 'Categorical, Integer', 'Real', 'Real', 'Categorical, Integer, Real', 'Categorical', '-', 'Integer', 'Integer', 'Categorical, Integer, Real', 'Categorical, Real', 'Integer, Real', 'Categorical, Integer, Real', 'Real', 'Real', 'Categorical', 'Real', 'Categorical, Integer', 'Integer', 'Categorical, Integer', 'Categorical', 'Real', 'Real', 'Integer, Real', 'Categorical', 'Integer', 'Categorical', 'Real', '-', '-', 'Real', 'Categorical', 'Real', 'Integer', 'Integer', 'Categorical, Integer', 'Integer'] 100\n",
      "------------------\n",
      "['Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification, Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification, Regression', 'Classification', 'Classification, Clustering', 'Classification', 'Regression', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification, Clustering', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Regression, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Regression', 'Classification, Regression, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Regression', 'Classification', 'Regression', 'Classification', 'Regression', 'Classification, Regression, Clustering', 'Classification', 'Classification, Clustering', 'Classification', 'Classification', 'Regression', 'Classification', 'Regression', 'Classification', 'Regression', 'Regression', 'Classification', 'Classification', 'Classification', 'Classification, Regression, Clustering', 'Classification', 'Regression, Clustering', 'Classification', 'Classification', '-', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Regression', 'Classification', 'Classification, Regression', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression', 'Regression', 'Classification, Regression', 'Classification', 'Classification, Regression, Clustering', 'Regression', 'Classification', '-', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Classification', 'Regression, Description', 'Classification'] 100\n",
      "------------------\n",
      "['Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics', 'Dataset Characteristics'] 100\n",
      "------------------\n",
      "['150', '13611', '303', '3810', '900', '48842', '569', '178', '4898', '45211', '1', '1728', '649', '4177', '8124', '541909', '1000', '205', '4424', '48842', '1', '1728', '649', '4177', '8124', '541909', '1000', '205', '4424', '48842', '1067371', '5574', '7200', '2111', '5620', '2075259', '19020', '12330', '101', '517', '299', '581012', '101766', '253680', '32', '345', '20000', '414', '351', '209', '299', '581012', '101766', '253680', '32', '345', '20000', '414', '351', '209', '1030', '197', '1484', '958', '10000', '2310', '370', '452', '12960', '20000', '11055', '10992', '690', '270', '39797', '122', '1372', '19735', '625', '198', '366', '145', '520', '307', '1994', '9568', '1197', '714', '215063', '1389', '336', '10000', '3000', '11000000', '21578', '1885', '946', '6435', '9000', '58000'] 100\n",
      "------------------\n",
      "['4', '16', '13', '7', '7', '14', '30', '13', '11', '16', '20', '6', '30', '8', '22', '6', '20', '25', '36', '14', '20', '6', '30', '8', '22', '6', '20', '25', '36', '14', '-', '-', '5', '16', '64', '9', '10', '17', '16', '12', '12', '54', '47', '21', '56', '5', '16', '6', '34', '10', '12', '54', '47', '21', '56', '5', '16', '6', '34', '10', '8', '22', '8', '9', '21', '19', '140256', '279', '8', '-', '30', '16', '14', '13', '58', '-', '4', '28', '4', '33', '34', '31', '16', '35', '127', '4', '14', '14', '6', '10', '7', '-', '-', '-', '5', '12', '18', '36', '-', '7'] 100\n",
      "------------------\n",
      "['Donated on 6/30/1988', 'Donated on 9/13/2020', 'Donated on 6/30/1988', 'Donated on 10/5/2019', 'Donated on 8/13/2023', 'Donated on 4/30/1996', 'Donated on 10/31/1995', 'Donated on 6/30/1991', 'Donated on 10/6/2009', 'Donated on 2/13/2012', 'Donated on 5/31/1997', 'Donated on 11/26/2014', 'Donated on 11/30/1995', 'Donated on 4/26/1987', 'Donated on 11/5/2015', 'Donated on 11/16/1994', 'Donated on 5/18/1987', 'Donated on 12/12/2021', 'Donated on 4/30/1996', 'Donated on 5/31/1997', 'Donated on 11/26/2014', 'Donated on 11/30/1995', 'Donated on 4/26/1987', 'Donated on 11/5/2015', 'Donated on 11/16/1994', 'Donated on 5/18/1987', 'Donated on 12/12/2021', 'Donated on 4/30/1996', 'Donated on 9/20/2019', 'Donated on 6/21/2012', 'Donated on 12/31/1986', 'Donated on 8/26/2019', 'Donated on 6/30/1998', 'Donated on 8/29/2012', 'Donated on 4/30/2007', 'Donated on 8/30/2018', 'Donated on 5/14/1990', 'Donated on 2/28/2008', 'Donated on 2/4/2020', 'Donated on 7/31/1998', 'Donated on 5/2/2014', 'Linked on 9/25/2023', 'Donated on 4/30/1992', 'Donated on 5/14/1990', 'Donated on 12/31/1990', 'Donated on 8/17/2018', 'Donated on 12/31/1988', 'Donated on 9/30/1987', 'Donated on 2/4/2020', 'Donated on 7/31/1998', 'Donated on 5/2/2014', 'Linked on 9/25/2023', 'Donated on 4/30/1992', 'Donated on 5/14/1990', 'Donated on 12/31/1990', 'Donated on 8/17/2018', 'Donated on 12/31/1988', 'Donated on 9/30/1987', 'Donated on 8/2/2007', 'Donated on 6/25/2008', 'Donated on 8/31/1996', 'Donated on 8/18/1991', 'Donated on 12/25/2019', 'Donated on 10/31/1990', 'Donated on 3/12/2015', 'Donated on 12/31/1997', 'Donated on 5/31/1997', 'Donated on 9/8/1999', 'Donated on 3/25/2015', 'Donated on 6/30/1998', 'Donated on 5/30/2015', 'Donated on 10/12/1999', 'Donated on 4/15/2013', 'Donated on 2/14/2017', 'Donated on 4/21/1994', 'Donated on 11/30/1995', 'Donated on 12/31/1997', 'Donated on 8/14/2023', 'Donated on 7/11/2020', 'Donated on 7/10/1988', 'Donated on 7/12/2009', 'Donated on 3/25/2014', 'Donated on 8/2/2020', 'Donated on 12/5/2023', 'Donated on 10/3/2018', 'Donated on 2/28/1989', 'Donated on 8/31/1996', 'Donated on 7/6/1999', 'Donated on 5/29/2015', 'Donated on 2/11/2014', 'Donated on 9/25/1997', 'Donated on 10/16/2016', 'Donated on 2/12/1993', 'Donated on 7/2/2000'] 94\n"
     ]
    }
   ],
   "source": [
    "dataset=[]\n",
    "type=[]\n",
    "task=[]\n",
    "att_type=[]\n",
    "instance=[]\n",
    "attr_no=[]\n",
    "year=[]\n",
    "try:\n",
    "    for link in url:\n",
    "        driver.get(link)\n",
    "        ds=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[1]/div[2]/div/h1')\n",
    "        tp=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[4]/p')\n",
    "        tk=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[3]/p')\n",
    "        att_tp=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[1]/h1')\n",
    "        ins=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[5]/p')\n",
    "        att_no=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[2]/div[2]/div[6]/p')\n",
    "        yr=driver.find_elements(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div[1]/div[1]/div[2]/h2')\n",
    "        \n",
    "        try:\n",
    "            for i in ds:\n",
    "                if i is not None:\n",
    "                    dataset.append(i.text)        \n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "            \n",
    "        try:\n",
    "            for i in tp:\n",
    "                if i is not None:\n",
    "                    type.append(i.text)\n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        try:\n",
    "            for i in tk:\n",
    "                if i is not None:\n",
    "                    task.append(i.text)\n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        try:\n",
    "            for i in att_tp:\n",
    "                if i is not None:\n",
    "                    att_type.append(i.text)        \n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        try:\n",
    "            for i in ins:\n",
    "                if i is not None:\n",
    "                    instance.append(i.text)\n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        try:\n",
    "            for i in att_no:\n",
    "                if i is not None:\n",
    "                    attr_no.append(i.text)\n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        try:\n",
    "            for i in yr:\n",
    "                if i is not None:\n",
    "                    year.append(i.text) \n",
    "        except NoElementFoundException:        \n",
    "            print(\"NoElementFoundException Error\")\n",
    "        \n",
    "except ElementClickInterceptedException:\n",
    "    print(\"ElementClickInterceptedException Error\")\n",
    "    \n",
    "print(dataset,len(dataset)) \n",
    "print(\"------------------\")\n",
    "print(type,len(type)) \n",
    "print(\"------------------\")\n",
    "print(task,len(task)) \n",
    "print(\"------------------\")\n",
    "print(att_type,len(task)) \n",
    "print(\"------------------\")\n",
    "print(instance,len(instance)) \n",
    "print(\"------------------\")\n",
    "print(attr_no,len(attr_no)) \n",
    "print(\"------------------\")\n",
    "print(year,len(year)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa654dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>Instance Number</th>\n",
       "      <th>Attribute Number</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>Donated on 6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dry Bean</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>13611</td>\n",
       "      <td>16</td>\n",
       "      <td>Donated on 9/13/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>303</td>\n",
       "      <td>13</td>\n",
       "      <td>Donated on 6/30/1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>3810</td>\n",
       "      <td>7</td>\n",
       "      <td>Donated on 10/5/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Raisin</td>\n",
       "      <td>Real, Integer</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>900</td>\n",
       "      <td>7</td>\n",
       "      <td>Donated on 8/13/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Combined Cycle Power Plant</td>\n",
       "      <td>Real</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>9568</td>\n",
       "      <td>4</td>\n",
       "      <td>Donated on 2/28/1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Productivity Prediction of Garment Employees</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>1197</td>\n",
       "      <td>14</td>\n",
       "      <td>Donated on 8/31/1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>National Poll on Healthy Aging (NPHA)</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>714</td>\n",
       "      <td>14</td>\n",
       "      <td>Donated on 7/6/1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Drug Reviews (Drugs.com)</td>\n",
       "      <td>Integer</td>\n",
       "      <td>Classification, Regression, Clustering</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>215063</td>\n",
       "      <td>6</td>\n",
       "      <td>Donated on 5/29/2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Solar Flare</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Dataset Characteristics</td>\n",
       "      <td>1389</td>\n",
       "      <td>10</td>\n",
       "      <td>Donated on 2/11/2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Dataset Name                   Data Type  \\\n",
       "0                                           Iris                        Real   \n",
       "1                                       Dry Bean               Integer, Real   \n",
       "2                                  Heart Disease  Categorical, Integer, Real   \n",
       "3                     Rice (Cammeo and Osmancik)                        Real   \n",
       "4                                         Raisin               Real, Integer   \n",
       "..                                           ...                         ...   \n",
       "85                    Combined Cycle Power Plant                        Real   \n",
       "86  Productivity Prediction of Garment Employees               Integer, Real   \n",
       "87         National Poll on Healthy Aging (NPHA)                 Categorical   \n",
       "88                      Drug Reviews (Drugs.com)                     Integer   \n",
       "89                                   Solar Flare                 Categorical   \n",
       "\n",
       "                                      Task           Attribute Type  \\\n",
       "0                           Classification  Dataset Characteristics   \n",
       "1                           Classification  Dataset Characteristics   \n",
       "2                           Classification  Dataset Characteristics   \n",
       "3                           Classification  Dataset Characteristics   \n",
       "4                           Classification  Dataset Characteristics   \n",
       "..                                     ...                      ...   \n",
       "85                              Regression  Dataset Characteristics   \n",
       "86              Classification, Regression  Dataset Characteristics   \n",
       "87                          Classification  Dataset Characteristics   \n",
       "88  Classification, Regression, Clustering  Dataset Characteristics   \n",
       "89                              Regression  Dataset Characteristics   \n",
       "\n",
       "   Instance Number Attribute Number                  Year  \n",
       "0              150                4  Donated on 6/30/1988  \n",
       "1            13611               16  Donated on 9/13/2020  \n",
       "2              303               13  Donated on 6/30/1988  \n",
       "3             3810                7  Donated on 10/5/2019  \n",
       "4              900                7  Donated on 8/13/2023  \n",
       "..             ...              ...                   ...  \n",
       "85            9568                4  Donated on 2/28/1989  \n",
       "86            1197               14  Donated on 8/31/1996  \n",
       "87             714               14   Donated on 7/6/1999  \n",
       "88          215063                6  Donated on 5/29/2015  \n",
       "89            1389               10  Donated on 2/11/2014  \n",
       "\n",
       "[90 rows x 7 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Dataset Name':dataset[:90],'Data Type':type[:90],'Task':task[:90],'Attribute Type':att_type[:90],'Instance Number':instance[:90],\n",
    "                 'Attribute Number':attr_no[:90],'Year':year[:90]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a5302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
